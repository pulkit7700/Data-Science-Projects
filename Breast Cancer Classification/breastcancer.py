# -*- coding: utf-8 -*-
"""BreastCancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HjOhbtfeHGdk3xT6xvS8cMcmEATr3oL8
"""

import pandas as pd
import numpy as  np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('data.csv')
df.dropna()

sns.countplot(df.diagnosis, label="count")

df["diagnosis"].value_counts()

df.info()

x = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

y = y.reshape(-1, 1)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import LabelEncoder, StandardScaler
sc = StandardScaler()
le = LabelEncoder()
y_train = le.fit_transform(y_train)
x_train = sc.fit_transform(x_train)

y_test = le.fit_transform(y_test)

y_train

"""# Fitting the Model"""

def models(X_train, Y_train):

    # Using Logistic Regression
    from sklearn.linear_model import LogisticRegression
    log = LogisticRegression(random_state=0)
    log.fit(X_train, Y_train)

    # Using KNeighborsClassifier
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
    knn.fit(X_train, Y_train)

    # Using SVC linear
    from sklearn.svm import SVC
    svc_lin = SVC(kernel='linear', random_state=0)
    svc_lin.fit(X_train, Y_train)

    # Using SVC rbf
    from sklearn.svm import SVC
    svc_rbf = SVC(kernel='rbf', random_state=0)
    svc_rbf.fit(X_train, Y_train)

    # Using GaussianNB
    from sklearn.naive_bayes import GaussianNB
    gauss = GaussianNB()
    gauss.fit(X_train, Y_train)

    # Using DecisionTreeClassifier
    from sklearn.tree import DecisionTreeClassifier
    tree = DecisionTreeClassifier(criterion='entropy', random_state=0)
    tree.fit(X_train, Y_train)

    # Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm
    from sklearn.ensemble import RandomForestClassifier
    forest = RandomForestClassifier(
        n_estimators=10, criterion='entropy', random_state=0)
    forest.fit(X_train, Y_train)

    # print model accuracy on the training data.
    print('[0]Logistic Regression Training Accuracy:',
          log.score(X_train, Y_train))
    print('[1]K Nearest Neighbor Training Accuracy:',
          knn.score(X_train, Y_train))
    print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:',
          svc_lin.score(X_train, Y_train))
    print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:',
          svc_rbf.score(X_train, Y_train))
    print('[4]Gaussian Naive Bayes Training Accuracy:',
          gauss.score(X_train, Y_train))
    print('[5]Decision Tree Classifier Training Accuracy:',
          tree.score(X_train, Y_train))
    print('[6]Random Forest Classifier Training Accuracy:',
          forest.score(X_train, Y_train))

    return log, knn, svc_lin, svc_rbf, gauss, tree, forest

model = models(x_train, y_train)

from sklearn.model_selection import cross_val_score

def scores(X_train, y_train):
    from sklearn.linear_model import LogisticRegression
    log = LogisticRegression()
    log_scores =  cross_val_score(log, X_train, y_train, cv=10)
    
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier()
    knn_scores = cross_val_score(knn, X_train, y_train, cv=10)


    from sklearn.svm import SVC
    svm_lin = SVC(kernel="linear")
    svm_lin_scores = cross_val_score(svm_lin, X_train, y_train, cv=10)

    from sklearn.svm import SVC
    svm_rbf = SVC(kernel='rbf')
    svm_rbf_scores = cross_val_score(svm_rbf, X_train, y_train, cv=10)

    from sklearn.naive_bayes import GaussianNB
    gauss = KNeighborsClassifier()
    gauss_scores = cross_val_score(gauss, X_train, y_train, cv=10)

    from sklearn.tree import DecisionTreeClassifier
    tree = DecisionTreeClassifier()
    tree_scores = cross_val_score(tree, X_train, y_train, cv=10)

    from sklearn.ensemble import RandomForestClassifier
    forest= RandomForestClassifier()
    forest_scores = cross_val_score(forest, X_train, y_train, cv=10)

    print('[0]Logistic Regression Training Scores:', log_scores.mean())
    print('[1]K Nearest Neighbor Training Scores:', knn_scores.mean())
    print('[2]Support Vector Machine (Linear Classifier) Training Scores:', svm_lin_scores.mean())
    print('[3]Support Vector Machine (RBF Classifier) Training Scores:', svm_rbf_scores.mean())
    print('[4]Gaussian Naive Bayes Training Scores:', gauss_scores.mean())
    print('[5]Decision Tree Classifier Training Scores:', tree_scores.mean())
    print('[6]Random Forest Classifier Training Scores:', forest_scores.mean())
  
    return log_scores, knn_scores, svm_lin_scores, svm_rbf_scores, gauss_scores, tree_scores, forest_scores

score = scores(x_train, y_train)

plt.figure(figsize=(8, 4))
plt.plot([0]*10, score[0], ".")
plt.plot([1]*10, score[1], ".")
plt.plot([2]*10, score[2], ".")
plt.plot([3]*10, score[3], ".")
plt.plot([4]*10, score[4], ".")
plt.plot([5]*10, score[5], ".")
plt.plot([6]*10, score[6], ".")
plt.boxplot([score[0], score[1], score[2],score[3],score[4],score[5],score[6]], labels=("logistic", "Kneighbors","SVM_L","SVM_L","GNB","DTC","Random Forest"))
plt.ylabel("Accuracy", fontsize=14)
plt.show()

"""# Confussion Metrics"""

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, model[0].predict(x_test))

print(cm)

for i in range(len(model)):
  cm = confusion_matrix(y_test, model[i].predict(x_test))
  
  TN = cm[0][0]
  TP = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]
  
  print(cm)
  print('Model[{}] Testing Accuracy = "{}!"'.format(i,  (TP + TN) / (TP + TN + FN + FP)))
  print()

"""# Accuracy Scores"""

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model ',i)
  #Check precision, recall, f1-score
  print( classification_report(y_test, model[i].predict(x_test)) )
  #Another way to get the models accuracy on the test data
  print( accuracy_score(y_test, model[i].predict(x_test)))
  print()

y_pred = model[5].predict(x_test)

"""# Creating Dataframes for Viewing results

Aligning the test
"""

y_pred1 = y_pred.reshape(-1, 1)
y_test1 = y_test.reshape(-1, 1)

# Putting the predicted and test in the same numpy array
Collection = np.concatenate((y_pred1.reshape(len(y_pred1),1), y_test1.reshape(len(y_test1),1)),1)
View = pd.DataFrame(Collection, columns=["Predicted", "Actual"])
View

"""# Actual Values"""

## With Actual Values 

Test1 = le.inverse_transform(y_pred1)
Test2 = le.inverse_transform(y_test1)
Collection2 = np.concatenate((Test1.reshape(len(Test1),1), Test2.reshape(len(Test2),1)),1)
View2 = pd.DataFrame(Collection2, columns=["Predicted", "Actual"])
View2

sm2 = confusion_matrix(y_test, y_pred)
sm2

"""# Cumulative Frequency  Curve """

total =  len(y_test)
count_1 = np.sum(y_test)
count_0 = total - count_1
plt.figure(figsize=(20,12))

probs = dtc.predict_proba(x_test)
probs = probs[:, 1]
model_y = [y for _, y in sorted(zip(probs, y_test), reverse = True)]
y_values = np.append([0], np.cumsum(model_y))
x_values = np.arange(0, total + 1)

plt.plot([0, total], [0, count_1], c = 'r', linestyle = '--', label = 'Random Model')

plt.plot([0, count_1, total], 
         [0, count_1, count_1], 
         c = 'grey', 
         linewidth = 2, 
         label = 'Perfect Model')

plt.plot(x_values, 
         y_values, 
         c = 'b', 
         label = 'Support Vector Classifier', 
         linewidth = 4)

# Plot information
plt.xlabel('Total observations', fontsize = 16)
plt.ylabel('Class 1 observations', fontsize = 16)
plt.title('Cumulative Accuracy Profile', fontsize = 16)
plt.legend(loc = 'lower right', fontsize = 16)

"""# Finished"""